{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Autodistill uses big, slower foundation models to train small, faster supervised models. Using <code>autodistill</code>, you can go from unlabeled images to inference on a custom model running at the edge with no human intervention in between.</p> <p> </p> <p>Here are example predictions of a Target Model detecting milk bottles and bottlecaps after being trained on an auto-labeled dataset using Autodistill (see the Autodistill YouTube video for a full walkthrough):</p> <p> </p>"},{"location":"#features","title":"\ud83d\ude80 Features","text":"<ul> <li>\ud83d\udd0c Pluggable interface to connect models together</li> <li>\ud83e\udd16 Automatically label datasets</li> <li>\ud83d\udc30 Train fast supervised models</li> <li>\ud83d\udd12 Own your model</li> <li>\ud83d\ude80 Deploy distilled models to the cloud or the edge</li> </ul>"},{"location":"#basic-concepts","title":"\ud83d\udcda Basic Concepts","text":"<p>To use <code>autodistill</code>, you input unlabeled data into a Base Model which uses an Ontology to label a Dataset that is used to train a Target Model which outputs a Distilled Model fine-tuned to perform a specific Task.</p> <p> </p> <p>Autodistill defines several basic primitives:</p> <ul> <li>Task - A Task defines what a Target Model will predict. The Task for each component (Base Model, Ontology, and Target Model) of an <code>autodistill</code> pipeline must match for them to be compatible with each other. Object Detection and Instance Segmentation are currently supported through the <code>detection</code> task. <code>classification</code> support will be added soon.</li> <li>Base Model - A Base Model is a large foundation model that knows a lot about a lot. Base models are often multimodal and can perform many tasks. They're large, slow, and expensive. Examples of Base Models are GroundedSAM and GPT-4's upcoming multimodal variant. We use a Base Model (along with unlabeled input data and an Ontology) to create a Dataset.</li> <li>Ontology - an Ontology defines how your Base Model is prompted, what your Dataset will describe, and what your Target Model will predict. A simple Ontology is the <code>CaptionOntology</code> which prompts a Base Model with text captions and maps them to class names. Other Ontologies may, for instance, use a CLIP vector or example images instead of a text caption.</li> <li>Dataset - a Dataset is a set of auto-labeled data that can be used to train a Target Model. It is the output generated by a Base Model.</li> <li>Target Model - a Target Model is a supervised model that consumes a Dataset and outputs a distilled model that is ready for deployment. Target Models are usually small, fast, and fine-tuned to perform a specific task very well (but they don't generalize well beyond the information described in their Dataset). Examples of Target Models are YOLOv8 and DETR.</li> <li>Distilled Model - a Distilled Model is the final output of the <code>autodistill</code> process; it's a set of weights fine-tuned for your task that can be deployed to get predictions.</li> </ul>"},{"location":"#theory-and-limitations","title":"\ud83d\udca1 Theory and Limitations","text":"<p>Human labeling is one of the biggest barriers to broad adoption of computer vision. It can take thousands of hours to craft a dataset suitable for training a production model. The process of distillation for training supervised models is not new, in fact, traditional human labeling is just another form of distillation from an extremely capable Base Model (the human brain \ud83e\udde0).</p> <p>Foundation models know a lot about a lot, but for production we need models that know a lot about a little.</p> <p>As foundation models get better and better they will increasingly be able to augment or replace humans in the labeling process. We need tools for steering, utilizing, and comparing these models. Additionally, these foundation models are big, expensive, and often gated behind private APIs. For many production use-cases, we need models that can run cheaply and in realtime at the edge.</p> <p> </p> <p>Autodistill's Base Models can already create datasets for many common use-cases (and through creative prompting and few-shotting we can expand their utility to many more), but they're not perfect yet. There's still a lot of work to do; this is just the beginning and we'd love your help testing and expanding the capabilities of the system!</p>"},{"location":"#installation","title":"\ud83d\udcbf Installation","text":"<p>Autodistill is modular. You'll need to install the <code>autodistill</code> package (which defines the interfaces for the above concepts) along with Base Model and Target Model plugins (which implement specific models).</p> <p>By packaging these separately as plugins, dependency and licensing incompatibilities are minimized and new models can be implemented and maintained by anyone.</p> <p>Example:  <pre><code>pip install autodistill autodistill-grounded-sam autodistill-yolov8\n</code></pre></p> Install from source  You can also clone the project from GitHub for local development:  <pre><code>git clone https://github.com/roboflow/autodistill\ncd autodistill\npip install -e .\n</code></pre> <p>Additional Base and Target models are enumerated below.</p>"},{"location":"#video-guides","title":"\ud83c\udfac Video Guides","text":"<p> Autodistill: Train YOLOv8 with ZERO Annotations Published: 8 June 2023 In this video, we will show you how to use a new library to train a YOLOv8 model to detect bottles moving on a conveyor line. Yes, that's right - zero annotation hours are required! We dive deep into Autodistill's functionality, covering topics from setting up your Python environment and preparing your images, to the thrilling automatic annotation of images. </p>"},{"location":"#community-resources","title":"\ud83d\udca1 Community Resources","text":"<ul> <li>Distill Large Vision Models into Smaller, Efficient Models with Autodistill: Announcement post with written guide on how to use Autodistill</li> <li>Comparing AI-Labeled Data to Human-Labeled Data: A qualitative evaluation of Grounding DINO used with Autodistill across various tasks and domains.</li> <li>How to Evaluate Autodistill Prompts with CVevals: Evaluate Autodistill prompts.</li> <li>Autodistill: Label and Train a Computer Vision Model in Under 20 Minutes: Building a model to detect planes in under 20 minutes.</li> <li>Comparing AI-Labeled Data to Human-Labeled Data: Explore the strengths and limitations of a base model used with Autoditsill.</li> <li>Train an Image Classification Model with No Labeling: Use Grounded SAM to automatically label images for training an Ultralytics YOLOv8 classification model.</li> <li>Train a Segmentation Model with No Labeling: Use CLIP to automatically label images for training an Ultralytics YOLOv8 segmentation model.</li> <li>File a PR to add your own resources here!</li> </ul>"},{"location":"#roadmap","title":"\ud83d\uddfa\ufe0f Roadmap","text":"<p>Apart from adding new models, there are several areas we plan to explore with <code>autodistill</code> including:</p> <ul> <li>\ud83d\udca1 Ontology creation &amp; prompt engineering</li> <li>\ud83d\udc69\u200d\ud83d\udcbb Human in the loop support</li> <li>\ud83e\udd14 Model evaluation</li> <li>\ud83d\udd04 Active learning</li> <li>\ud83d\udcac Language tasks</li> </ul>"},{"location":"#contributing","title":"\ud83c\udfc6 Contributing","text":"<p>We love your input! Please see our contributing guide to get started. Thank you \ud83d\ude4f to all our contributors!</p>"},{"location":"#license","title":"\ud83d\udc69\u200d\u2696\ufe0f License","text":"<p>The <code>autodistill</code> package is licensed under an Apache 2.0. Each Base or Target model plugin may use its own license corresponding with the license of its underlying model. Please refer to the license in each plugin repo for more information.</p>"},{"location":"#frequently-asked-questions","title":"Frequently Asked Questions \u2753","text":""},{"location":"#what-causes-the-pytorchstreamreader-failed-reading-zip-archive-failed-finding-central-directory-error","title":"What causes the <code>PytorchStreamReader failed reading zip archive: failed finding central directory</code> error?","text":"<p>This error is caused when PyTorch cannot load the model weights for a model. Go into the <code>~/.cache/autodistill</code> directory and delete the folder associated with the model you are trying to load. Then, run your code again. The model weights will be downloaded from scratch. Leave the installation process uninterrupted.</p>"},{"location":"#explore-more-roboflow-open-source-projects","title":"\ud83d\udcbb explore more Roboflow open source projects","text":"Project Description supervision General-purpose utilities for use in computer vision projects, from predictions filtering and display to object tracking to model evaluation. Autodistill (this project) Automatically label images for use in training computer vision models. Inference An easy-to-use, production-ready inference server for computer vision supporting deployment of many popular model architectures and fine-tuned models. Notebooks Tutorials for computer vision tasks, from training state-of-the-art models to tracking objects to counting objects in a zone. Collect Automated, intelligent data collection powered by CLIP."},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p>"},{"location":"changelog/#010-2023-05-17","title":"[0.1.0] - 2023-05-17","text":"<p>Launched the <code>autodistill</code> package with support for GroundingSAM.</p>"},{"location":"command-line-interface/","title":"Command Line Interface","text":"<p>You can use Autodistill with a command line interface (CLI). </p> <p>The CLI allows you to run inference on a model or auto-label a folder of imageswihout writing a labeling script.</p>"},{"location":"command-line-interface/#installation","title":"Installation","text":"<p>To install the CLI, install the Autodistill Python package:</p> <pre><code>pip install autodistill\n</code></pre> <p>The CLI accepts several arguments:</p> <ul> <li><code>images</code>: The path to the folder of images you want to label.</li> <li><code>--base</code>: The base model you want to use for labeling. This can be any model from the Autodistill Model Zoo.</li> <li><code>--target</code>: The target model you want to use to train a model with your labeled dataset.</li> <li><code>--ontology</code>: The ontology you want to use for labeling. This must be a mapping of text prompts to send to a model to the label you want to save in your dataset. For example, <code>{\"acoustic guitar\": \"guitar\"}</code> will send the text prompt <code>acoustic guitar</code> to the model, then save the label as <code>guitar</code> in your dataset.</li> <li><code>--output</code>: The path to the folder where you want to save your labeled dataset.</li> </ul> <p>Here is an example:</p> <pre><code>autodistill images --base=\"grounding_dino\" --target=\"yolov8\" --ontology '{\"prompt\": \"label\"}' --output=\"./dataset\"\n</code></pre> <p>This command will label all images in a directory called <code>images</code> with Grounding DINO and use the labeled images to train a YOLOv8 model. Grounding DINO will label all images with the \"prompt\" and save the label as the \"label\". You can specify as many prompts and labels as you want. The resulting dataset will be saved in a folder called <code>dataset</code>.</p>"},{"location":"image-loading/","title":"Image Loading","text":"<p>All Autodistill base models (i.e. Grounding DINO or CLIP) support providing a file name and loading the corresponding image for use in labeling. Some models also enable passing images directly from the following formats:</p> <ul> <li>PIL <code>Image</code></li> <li>cv2 image</li> <li>URL, from which an image is retrieved</li> <li>A file name, which is loaded as an image</li> </ul> <p>This is handled by the low-level <code>load_image</code> function. This function allows you to pass any of the above formats. The PIL and cv2 formats are ideal if you already have an image in memory. Base models use this function to request the format the model needs. If a model needs an image in a format different from what you have provided -- for example, if you provided a file name and the model needs a PIL <code>Image</code> object -- the <code>load_image</code> function will convert the image to the correct format.</p> <p>The following models support the <code>load_image</code> function. The <code>PIL</code> and <code>cv2</code> states to what format <code>load_image</code> will convert your image (if necessary) to pass your image into a model.</p> <ul> <li>AltCLIP: PIL</li> <li>CLIP: PIL</li> <li>Grounding DINO: cv2</li> <li>MetaCLIP: PIL</li> <li>RemoteCLIP: PIL</li> <li>Transformers: PIL</li> <li>SAM HQ: cv2</li> <li>Segment Anything: cv2</li> <li>DETIC: PIL</li> <li>VLPart: PIL</li> <li>CoDet: PIL</li> <li>OWLv2: PIL</li> <li>FastViT: PIL</li> <li>FastSAM: cv2</li> <li>SegGPT: PIL</li> <li>OWLViT: PIL</li> <li>BLIPv2: PIL</li> <li>DINOv2: PIL</li> <li>Grounded SAM: cv2</li> <li>BLIP: PIL</li> </ul>"},{"location":"image-loading/#load_image-function","title":"<code>load_image</code> function","text":"<p>Load an image from a file path, URI, PIL image, or numpy array.</p> <p>This function is for use by Autodistill modules. You don't need to use it directly.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image to load</p> required <code>return_format</code> <p>The format to return the image in</p> <code>'cv2'</code> <p>Returns:</p> Type Description <code>Any</code> <p>The image in the specified format</p> Source code in <code>autodistill/helpers.py</code> <pre><code>def load_image(\n    image: Any,\n    return_format=\"cv2\",\n) -&gt; Any:\n\"\"\"\n    Load an image from a file path, URI, PIL image, or numpy array.\n\n    This function is for use by Autodistill modules. You don't need to use it directly.\n\n    Args:\n        image: The image to load\n        return_format: The format to return the image in\n\n    Returns:\n        The image in the specified format\n    \"\"\"\n    if return_format not in ACCEPTED_RETURN_FORMATS:\n        raise ValueError(f\"return_format must be one of {ACCEPTED_RETURN_FORMATS}\")\n\n    if isinstance(image, Image.Image) and return_format == \"PIL\":\n        return image\n    elif isinstance(image, Image.Image) and return_format == \"cv2\":\n        # channels need to be reversed for cv2\n        return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n    elif isinstance(image, Image.Image) and return_format == \"numpy\":\n        return np.array(image)\n\n    if isinstance(image, np.ndarray) and return_format == \"PIL\":\n        return Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    elif isinstance(image, np.ndarray) and return_format == \"cv2\":\n        return image\n    elif isinstance(image, np.ndarray) and return_format == \"numpy\":\n        return image\n\n    if isinstance(image, str) and image.startswith(\"http\"):\n        if return_format == \"PIL\":\n            response = requests.get(image)\n            return Image.open(BytesIO(response.content))\n        elif return_format == \"cv2\" or return_format == \"numpy\":\n            response = requests.get(image)\n            pil_image = Image.open(BytesIO(response.content))\n            return np.array(pil_image)\n    elif os.path.isfile(image):\n        if return_format == \"PIL\":\n            return Image.open(image)\n        elif return_format == \"cv2\":\n            # channels need to be reversed for cv2\n            return cv2.cvtColor(np.array(Image.open(image)), cv2.COLOR_RGB2BGR)\n        elif return_format == \"numpy\":\n            pil_image = Image.open(image)\n            return np.array(pil_image)\n    else:\n        raise ValueError(f\"{image} is not a valid file path or URI\")\n</code></pre>"},{"location":"large-datasets/","title":"Labeling Large Datasets","text":"<p>Autodistill has not been optimized for labeling large datasets, but this work is in progress. In the mean time, we recommend labeling only a few hundred images at a time, to the maximum of how many images you can store in memory.</p>"},{"location":"large-datasets/#how-the-autodistill-labeling-process-works","title":"How the Autodistill labeling process works","text":"<p>During image labeling, a data structure is built that contains:</p> <ol> <li>A numpy representation of an image;</li> <li>The labels for the image, and;</li> <li>The image file name.</li> </ol> <p>If you are labeling large datasets, this data structure will get large. For example, if you have 10,000 images in a folder to label, this data structure will contain 10,000 images. This can cause memory issues if your system doesn't have enough memory to store all images.</p> <p>We are working on a system that will prevent the need to store images in memory during the labeling process. This system will also include an intelligent label resumption system, so if labeling stops for any reason you will be able to resume labeling from where you stopped.</p> <p>Follow Issue #93 in the Autodistill GitHub repository.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Autodistill lets you use large, foundation vision models to auto-label data for and train small, fine-tuned vision models. This process is called distillation.</p> <p>Your fine-tuned model will run smaller and faster, and will thus be more suitable for deployment on edge devices.</p>"},{"location":"quickstart/#how-autodistill-works","title":"How Autodistill Works","text":"<p>There are two main concepts in Autodistill:</p> <ul> <li>A base model, which is used to auto-label data. Examples include Grounding DINO, Grounded SAM, and CLIP.</li> <li>A target model, which is trained on the auto-labeled data. Examples include YOLOv5, YOLOv8, and DETR.</li> </ul> <p>You can use Autodistill with only a base model if you want to label data and run your own training.</p> <p>You can also use Autodistill with both a base model and a target model to build an end-to-end labeling and training pipeline.</p>"},{"location":"quickstart/#distill-a-model-tutorial","title":"Distill a Model (Tutorial)","text":"<p>Tip</p> <p>See the demo Notebook for a quick introduction to <code>autodistill</code>. This notebook walks through building a milk container detection model with no labeling.</p> <p>If you want to skip directly to the full code, without the tutorial, go to the Code Summary section.</p> <p>Let's distill a model to see how Autodistill works. We will use Autodistill to auto-label a milk bottle dataset.</p> <p>For this example, we'll show how to distill GroundedSAM into a small YOLOv8 model using autodistill-grounded-sam and autodistill-yolov8.</p>"},{"location":"quickstart/#step-1-install-autodistill-and-models","title":"Step #1: Install Autodistill and Models","text":"<p>First, install the required dependencies:</p> <pre><code>pip install autodistill autodistill-grounded-sam autodistill-yolov8\n</code></pre> <p>Tip</p> <p>See the Autodistill Supported Models list for a list of all supported models.</p>"},{"location":"quickstart/#step-2-set-an-ontology","title":"Step #2: Set an Ontology","text":"<p>Every base model needs an ontology. An ontology tells Autodistill what you want to identify and what labels should be called in your dataset.</p> <p>For example, if you want to identify milk bottles, you could use the following ontology:</p> <pre><code>{\n    \"milk bottle\": \"bottle\",\n    \"milk bottle cap\": \"bottle cap\"\n}\n</code></pre> <p>This ontology will tell Autodistill to identify milk bottles and milk bottle caps, and to save the labels as <code>bottle</code> and <code>bottle cap</code> in your dataset.</p>"},{"location":"quickstart/#step-3-set-up-the-model","title":"Step #3: Set up the Model","text":"<p>Let's set up our model. Create a new Python file and add the following lines of code:</p> <pre><code>from autodistill_grounded_sam import GroundedSAM\nfrom autodistill.detection import CaptionOntology\nfrom autodistill_yolov8 import YOLOv8\nfrom autodistill.utils import plot\nimport cv2\n\n# define an ontology to map class names to our GroundingDINO prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\nbase_model = GroundedSAM(ontology=CaptionOntology({\"milk bottle\": \"bottle\", \"milk bottle cap\": \"bottle cap\"}))\n</code></pre>"},{"location":"quickstart/#step-4-test-the-base-model","title":"Step #4: Test the Base Model","text":"<p>We can test our base model using the <code>predict</code> function:</p> <pre><code>results = base_model.predict(\"milk.jpg\")\n\nplot(\n    image=cv2.imread(\"milk.jpg\"),\n    classes=base_model.ontology.classes(),\n    detections=results\n)\n</code></pre>"},{"location":"quickstart/#step-5-label-a-dataset","title":"Step #5: Label a Dataset","text":"<p>Now that we have a base model, we can use it to label a dataset. You can label a dataset using the following code:</p> <pre><code>base_model.label_folder(\n    input_folder=\"./images\",\n    output_folder=\"./labeled-images\"\n)\n</code></pre>"},{"location":"quickstart/#step-6-train-a-target-model","title":"Step #6: Train a Target Model","text":"<p>We can use a target model like YOLOv8 to train a model on our labeled dataset. You can train a target model using the following code:</p> <pre><code>target_model = YOLOv8(\"yolov8n.pt\")\ntarget_model.train(\"./labeled-images/data.yaml\", epochs=200)\n</code></pre> <p>Your model weights will be saved in a folder called <code>runs</code>.</p> <p>For YOLOv8 models, you can then run inference locally using the ultralytics Python package, or deploy your model to Roboflow.</p>"},{"location":"quickstart/#code-summary","title":"Code Summary","text":"<p>Here is all of the code we used above, summarized into a single code snippet:</p> <pre><code>from autodistill_grounded_sam import GroundedSAM\nfrom autodistill.detection import CaptionOntology\nfrom autodistill_yolov8 import YOLOv8\n\n# define an ontology to map class names to our GroundingDINO prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n\nbase_model = GroundedSAM(ontology=CaptionOntology({\"milk bottle\": \"bottle\", \"milk bottle cap\": \"bottle cap\"}))\n\nresults = base_model.predict(\"milk.jpg\")\n\nbase_model.label_folder(\n    input_folder=\"./images\",\n    output_folder=\"./labeled-images\"\n)\n\ntarget_model = YOLOv8(\"yolov8n.pt\")\ntarget_model.train(\"./labeled-images/data.yaml\", epochs=200)\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<p>Above, we used Autodistill to label a dataset. Next, explore the Autodistill ecosystem of models following our guidance in the Which model should I use? guide. This site contains documentation for all Autodistill models, as well as utilities that you can use to work with each model.</p>"},{"location":"supported-models/","title":"Supported Models","text":"<p>Our goal is for <code>autodistill</code> to support using all foundation models as Base Models and most SOTA supervised models as Target Models. We focused on object detection and segmentation tasks first but plan to launch classification support soon! In the future, we hope <code>autodistill</code> will also be used for models beyond computer vision.</p> <ul> <li>\u2705 - complete (click row/column header to go to repo)</li> <li>\ud83d\udea7 - work in progress</li> </ul>"},{"location":"supported-models/#object-detection","title":"object detection","text":"base / target YOLOv8 YOLO-NAS YOLOv5 DETR YOLOv6 YOLOv7 MT-YOLOv6 DETIC \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 GroundedSAM \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 GroundingDINO \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 OWL-ViT \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 SAM-CLIP \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 LLaVA-1.5 \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 Kosmos-2 \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 OWLv2 \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 Roboflow Universe Models (50k+ pre-trained models) \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 CoDet \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 VLPart \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 Azure Custom Vision \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 AWS Rekognition \u2705 \u2705 \u2705 \u2705 \ud83d\udea7 Google Vision \u2705 \u2705 \u2705 \u2705 \ud83d\udea7"},{"location":"supported-models/#instance-segmentation","title":"instance segmentation","text":"base / target YOLOv8 YOLO-NAS YOLOv5 YOLOv7 Segformer GroundedSAM \u2705 \ud83d\udea7 \ud83d\udea7 SAM-CLIP \u2705 \ud83d\udea7 \ud83d\udea7 SegGPT \u2705 \ud83d\udea7 \ud83d\udea7 FastSAM \ud83d\udea7 \ud83d\udea7 \ud83d\udea7"},{"location":"supported-models/#classification","title":"classification","text":"base / target ViT YOLOv8 YOLOv5 CLIP \u2705 \u2705 \ud83d\udea7 MetaCLIP \u2705 \u2705 \ud83d\udea7 DINOv2 \u2705 \u2705 \ud83d\udea7 BLIP \u2705 \u2705 \ud83d\udea7 ALBEF \u2705 \u2705 \ud83d\udea7 FastViT \u2705 \u2705 \ud83d\udea7 AltCLIP \u2705 \u2705 \ud83d\udea7 Fuyu \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 Open Flamingo \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 GPT-4 PaLM-2"},{"location":"supported-models/#roboflow-model-deployment-support","title":"Roboflow Model Deployment Support","text":"<p>You can optionally deploy some Target Models trained using Autodistill on Roboflow. Deploying on Roboflow allows you to use a range of concise SDKs for using your model on the edge, from roboflow.js for web deployment to NVIDIA Jetson devices.</p> <p>The following Autodistill Target Models are supported by Roboflow for deployment:</p> model name Supported? YOLOv8 Object Detection \u2705 YOLOv8 Instance Segmentation \u2705 YOLOv5 Object Detection \u2705 YOLOv5 Instance Segmentation \u2705 YOLOv8 Classification"},{"location":"what-model-should-i-use/","title":"What Model Should I Use?","text":"<p>With so many base models to use in labeling images, you may wonder \"what model should I use for labeling?\"</p>"},{"location":"what-model-should-i-use/#detection-and-segmentation","title":"Detection and Segmentation","text":"<p>We recommend using Grounding DINO as a starting point for detection, and Grounded SAM for segmentation.</p> <p>Grounding DINO is an effective zero-shot object detector that can identify a wide range of objects, from cars to vinyl record covers.</p> <p>Grounded SAM combines SAM with Grounding DINO to generate segmentation masks from Grounding DINO predictions.</p> <p>If Grounding DINO does not identify the object you want to label, consider experimenting with DETIC, which can identify over 20,000 classes of objects. DETIC supports an open vocabulary, so you can provide arbitrary text labels for objects.</p>"},{"location":"what-model-should-i-use/#classification","title":"Classification","text":"<p>We recommend using CLIP as a starting point for classification, which is effective at classifying a wide range of objects. Read the CLIP abstract from OpenAI to learn more.</p>"},{"location":"what-model-should-i-use/#roboflow-universe-models","title":"Roboflow Universe Models","text":"<p>You can use any of the 50,000+ pre-trained models on Roboflow Universe to auto-label data. Universe covers an extensive range of models, covering areas from logistics to agriculture.</p> <p>See the <code>autodistill-roboflow-universe</code> base model for more information.</p>"},{"location":"what-model-should-i-use/#understanding-other-models","title":"Understanding Other Models","text":"<p>The guidance above is a starting point, but there are many other models from which you can choose.</p> <p>Below is a list of all supported models not covered above, as well as notes about their usage.</p> <p>Some models may no longer be recommended because a new model surpasses its performance.</p>"},{"location":"what-model-should-i-use/#detection","title":"Detection","text":"<ul> <li>LLaVA-1.5: LLaVA 1.5 has significant memory requirements compared to other models. It may generalize well to a wide range of objects due to its language grounding, but more experimentation is needed.</li> <li>Kosmos-2: Kosmos-2, like LLaVA-1.5, has significant memory requirements compared to other models.</li> <li>OWL-ViT: We recommend using OWLv2 over OWL-ViT.</li> <li>CoDet: CoDet is a promising zero-shot detection model which we encourage you to try if Grounding DINO does not identify the objects you want to label.</li> <li>VLPart: VLPart is a promising zero-shot detection model which we encourage you to try if Grounding DINO does not identify the objects you want to label.</li> </ul>"},{"location":"what-model-should-i-use/#classification_1","title":"Classification","text":"<ul> <li>FastViT: FastViT can identify the classes in the ImageNet 1k dataset. FastViT has fast inference times, which makes its use ideal in applications where inference speed is critical.</li> <li>AltCLIP: AltCLIP reports strong zero-shot classification performance in English and Chinese when evaluated against the ImageNet dataset. This model may be useful if you want to provide Chinese prompts to auto-label images.</li> <li>DINOv2: An embedding model that may be useful for zero-shot classification.</li> <li>MetaCLIP: MetaCLIP is an open source CLIP model. It may be worth experimenting with if OpenAI's CLIP model does not perform well on your dataset.</li> <li>BLIP: BLIP is a zero-shot classifier. It has higher memory requirements than CLIP, but may perform better on some datasets.</li> <li>ALBEF: ALBEF is a zero-shot classifier. It has higher memory requirements than CLIP, but may perform better on some datasets.</li> </ul>"},{"location":"base_models/albef/","title":"Albef","text":"<p>ALBEF, developed by Salesforce, is a computer vision model that supports a range of tasks, including image-text pre-training, image-text retrieval, visual question anserting, and zero-shot classification. You can classify images using ALBEF with Autodistill.</p>"},{"location":"base_models/albef/#installation","title":"Installation","text":"<p>To use ALBEF with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-albef\n</code></pre>"},{"location":"base_models/albef/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_albef import ALBEF\n\n# define an ontology to map class names to our ALBEF prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = ALBEF(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"a forklift\": \"forklift\"\n        }\n    )\n)\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/clip/","title":"CLIP","text":"<p>CLIP, developed by OpenAI, is a computer vision model trained using pairs of images and text. You can use CLIP with autodistill for image classification.</p> <p>This project is licensed under an MIT license.</p>"},{"location":"base_models/clip/#installation","title":"Installation","text":"<p>To use CLIP with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-clip\n</code></pre>"},{"location":"base_models/clip/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_clip import CLIP\nfrom autodistill.detection import CaptionOntology\n\n# define an ontology to map class names to our GroundingDINO prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = CLIP(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"a forklift\": \"forklift\"\n        }\n    )\n)\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/detic/","title":"DETIC","text":"<p>DETIC is a transformer-based object detection and segmentation model developed by Meta Research.</p>"},{"location":"base_models/detic/#installation","title":"Installation","text":"<p>To use DETIC with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-detic\n</code></pre>"},{"location":"base_models/detic/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_detic import DETIC\nfrom autodistill.detection import CaptionOntology\n\n# define an ontology to map class names to our DETIC prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = DETIC(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n        }\n    )\n)\nbase_model.label(\"./context_images\", extension=\".jpg\")\n</code></pre>"},{"location":"base_models/fastsam/","title":"FastSAM","text":"<p>FastSAM is a segmentation model trained on 2% of the SA-1B dataset used to train the Segment Anything Model.</p> <p>Read the full Autodistill documentation.</p> <p>Read the FastSAM Autodistill documentation.</p>"},{"location":"base_models/fastsam/#installation","title":"Installation","text":"<p>To use FastSAM with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-fastsam\n</code></pre>"},{"location":"base_models/fastsam/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_fastsam import FastSAM\n\n# define an ontology to map class names to our FastSAM prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = FastSAM(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"a forklift\": \"forklift\"\n        }\n    )\n)\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/groundedsam/","title":"Grounded SAM","text":"<p>Grounded SAM uses the Segment Anything Model to identify objects in an image and assign labels to each image.</p>"},{"location":"base_models/groundedsam/#installation","title":"Installation","text":"<p>To use the Grounded SAM base model, you will need to install the following dependency:</p> <pre><code>pip3 install autodistill-grounded-sam\n</code></pre>"},{"location":"base_models/groundedsam/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_grounded_sam import GroundedSAM\nfrom autodistill.detection import CaptionOntology\n\n\n# define an ontology to map class names to our GroundingDINO prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = GroundedSAM(ontology=CaptionOntology({\"shipping container\": \"container\"}))\n\n# label all images in a folder called `context_images`\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/grounding-dino/","title":"Grounding DINO","text":"<p>Grounding DINO is a zero-shot object detection model developed by IDEA Research. You can distill knowledge from Grounding DINO into a smaller model using Autodistill.</p>"},{"location":"base_models/grounding-dino/#installation","title":"Installation","text":"<p>To use the Grounded dino base model, you will need to install the following dependency:</p> <pre><code>pip3 install autodistill-grounding-dino\n</code></pre>"},{"location":"base_models/grounding-dino/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_grounding_dino import GroundingDINO\nfrom autodistill.detection import CaptionOntology\n\n\n# define an ontology to map class names to our GroundingDINO prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = GroundingDINO(ontology=CaptionOntology({\"shipping container\": \"container\"}))\n\n# label all images in a folder called `context_images`\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/owlvit/","title":"OWL-ViT","text":"<p>OWL-ViT is a transformer-based object detection model developed by Google Research.</p>"},{"location":"base_models/owlvit/#installation","title":"Installation","text":"<p>To use OWL-ViT with autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-owl-vit\n</code></pre>"},{"location":"base_models/owlvit/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_owl_vit import OWLViT\nfrom autodistill.detection import CaptionOntology\n\n# define an ontology to map class names to our OWLViT prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = OWLViT(\n    ontology=CaptionOntology(\n        {\n            \"person\": \"person\",\n            \"a forklift\": \"forklift\"\n        }\n    )\n)\nbase_model.label(\"./context_images\", extension=\".jpg\")\n</code></pre>"},{"location":"base_models/sam-clip/","title":"SAM-CLIP","text":"<p>SAM-CLIP uses the Segment Anything Model to identify objects in an image and assign labels to each image. Then, CLIP is used to find masks that are related to the given prompt.</p>"},{"location":"base_models/sam-clip/#installation","title":"Installation","text":"<p>To use the SAM-CLIP base model, you will need to install the following dependency:</p> <pre><code>pip3 install autodistill-sam-clip\n</code></pre>"},{"location":"base_models/sam-clip/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_sam_clip import SAMCLIP\nfrom autodistill.detection import CaptionOntology\n\n\n# define an ontology to map class names to our CLIP prompt\n# the ontology dictionary has the format {caption: class}\n# where caption is the prompt sent to the base model, and class is the label that will\n# be saved for that caption in the generated annotations\n# then, load the model\nbase_model = SAMCLIP(ontology=CaptionOntology({\"shipping container\": \"container\"}))\n\n# label all images in a folder called `context_images`\nbase_model.label(\"./context_images\", extension=\".jpeg\")\n</code></pre>"},{"location":"base_models/seggpt/","title":"SegGPT","text":"<p>SegGPT is a transformer-based, few-shot semantic segmentation model developed by BAAI Vision.</p> <p>This model performs well on task-specific segmentation tasks when given a few labeled images from which to learn features about the objects you want to identify.</p>"},{"location":"base_models/seggpt/#installation","title":"Installation","text":"<p>To use SegGPT with Autodistill, you need to install the following dependency:</p> <pre><code>pip3 install autodistill-seggpt\n</code></pre>"},{"location":"base_models/seggpt/#about-seggpt","title":"About SegGPT","text":"<p>SegGPT performs \"in-context\" segmentation. This means it requires a handful of pre-labelled \"context\" images.</p> <p>You will need some labeled images to use SegGPT. Don't have any labeled images? Check out Roboflow Annotate, a feature-rich annotation tool from which you can export data for use with Autodistill.</p>"},{"location":"base_models/seggpt/#quickstart","title":"Quickstart","text":"<pre><code>from autodistill_seggpt import SegGPT, FewShotOntology\n\nbase_model = SegGPT(\n    ontology=FewShotOntology(supervision_dataset)\n)\n\nbase_model.label(\"./unlabelled-climbing-photos\", extension=\".jpg\")\n</code></pre>"},{"location":"base_models/seggpt/#how-to-load-data-from-roboflow","title":"How to load data from Roboflow","text":"<p>Labelling and importing images is easy!</p> <p>You can use Roboflow Annotate to label a few images (5-10 should work fine). For your Project Type, make sure to pick Instance Segmentation, as you will be labelling with polygons.</p> <p>Once you have labelled your images, you can press Generate &gt; Generate New Version. You can use all the default options--no Augmentations are necessary.</p> <p>Once your dataset version is generated, you can press Export &gt; Continue.</p> <p>Then you will get some download code to copy. It should look something like this:</p> <pre><code>!pip install roboflow\n\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"ABCDEFG\")\nproject = rf.workspace(\"lorem-ipsum\").project(\"dolor-sit-amet\")\ndataset = project.version(1).download(\"yolov8\")\n</code></pre> <p>Note: if you are not using a notebook environment, you should remove <code>!pip install roboflow</code> from your code, and run <code>pip install roboflow</code> in your terminal instead.</p> <p>To import your dataset into Autodistill, run the following:</p> <pre><code>import supervision as sv\n\nsupervision_dataset = sv.DetectionDataset.from_yolo(\n    images_directory_path=f\"{dataset.location}/train/images\",\n    annotations_directory_path=f\"{dataset.location}/train/labels\",\n    data_yaml_path=f\"{dataset.location}/data.yaml\"\n)\n</code></pre>"},{"location":"reference/","title":"Index","text":"<p>This section of the Autodistill documentation covers the low-level Autodistill API. This section may be useful for advanced users who want to understand the Autodisitll API in more depth.</p>"},{"location":"reference/#autodistill-api-in-a-nutshell","title":"Autodistill API, in a Nutshell","text":"<p>The Autodistill API consists of three concepts:</p> <ol> <li>Base models, which are used to auto-label data;</li> <li>Target models, which are trained on the auto-labeled data; and</li> <li>Ontologies, which tell Autodistill what you want to identify and what labels should be called in your dataset.</li> </ol> <p>There are three different kinds of base models:</p> <ol> <li>Detection models, which identify objects in images and return bounding boxes and/or segmentation masks;</li> <li>Classification models, which classify images and return a class label; and;</li> <li>Embedding models, which return semantic embeddings for images. These are used with the <code>EmbeddingOntology</code> class.</li> </ol> <p>The base model you use depends on the type of data you want to label. You can also combine models using the ComposedDetectionModel API, which allows you to refine labels from detection models.</p> <p>There are two different kinds of target models:</p> <ol> <li>Detection models, and;</li> <li>Classification models.</li> </ol> <p>Autodistill does not support training embedding models.</p>"},{"location":"reference/utilities/","title":"Utilities","text":"<p>Learn about utility functions available for use with Autodistill.</p>"},{"location":"reference/utilities/#plot-an-image-with-predictions","title":"Plot an Image with Predictions","text":"<p>Plot bounding boxes or segmentation masks on an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p>The image to plot on</p> required <code>detections</code> <p>The detections to plot</p> required <code>classes</code> <code>List[str]</code> <p>The classes to plot</p> required <code>raw</code> <p>Whether to return the raw image or plot it interactively</p> <code>False</code> <p>Returns:</p> Type Description <p>The raw image (np.ndarray) if raw=True, otherwise None (image is plotted interactively</p> Source code in <code>autodistill/utils.py</code> <pre><code>def plot(image: np.ndarray, detections, classes: List[str], raw=False):\n\"\"\"\n    Plot bounding boxes or segmentation masks on an image.\n\n    Args:\n        image: The image to plot on\n        detections: The detections to plot\n        classes: The classes to plot\n        raw: Whether to return the raw image or plot it interactively\n\n    Returns:\n        The raw image (np.ndarray) if raw=True, otherwise None (image is plotted interactively\n    \"\"\"\n    # TODO: When we have a classification annotator\n    # in supervision, we can add it here\n    if detections.mask is not None:\n        annotator = sv.MaskAnnotator()\n    else:\n        annotator = sv.BoxAnnotator()\n\n    label_annotator = sv.LabelAnnotator()\n\n    labels = [\n        f\"{classes[class_id]} {confidence:0.2f}\"\n        for _, _, confidence, class_id, _ in detections\n    ]\n\n    annotated_frame = annotator.annotate(scene=image.copy(), detections=detections)\n    annotated_frame = label_annotator.annotate(\n        scene=annotated_frame, labels=labels, detections=detections\n    )\n\n    if raw:\n        return annotated_frame\n\n    sv.plot_image(annotated_frame, size=(8, 8))\n</code></pre>"},{"location":"reference/utilities/#compare-models","title":"Compare Models","text":"<p>Compare the predictions of multiple models on multiple images.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>list</code> <p>The models to compare</p> required <code>images</code> <code>List[str]</code> <p>The images to compare</p> required <p>Returns:</p> Type Description <p>A grid of images with the predictions of each model on each image.</p> Source code in <code>autodistill/utils.py</code> <pre><code>def compare(models: list, images: List[str]):\n\"\"\"\n    Compare the predictions of multiple models on multiple images.\n\n    Args:\n        models: The models to compare\n        images: The images to compare\n\n    Returns:\n        A grid of images with the predictions of each model on each image.\n    \"\"\"\n    image_results = []\n    model_results = []\n\n    for model in models:\n        # get model class name\n        model_name = model.__class__.__name__\n\n        for image in images:\n            results = model.predict(image)\n\n            image_data = cv2.imread(image)\n\n            image_result = plot(\n                image_data, results, classes=model.ontology.prompts(), raw=True\n            )\n\n            image_results.append(image_result)\n\n            model_results.append(model_name)\n\n    sv.plot_images_grid(\n        image_results,\n        grid_size=(len(models), len(images)),\n        titles=model_results,\n        size=(16, 16),\n    )\n</code></pre>"},{"location":"reference/utilities/#load-an-image","title":"Load an Image","text":"<p>Load an image from a file path, URI, PIL image, or numpy array.</p> <p>This function is for use by Autodistill modules. You don't need to use it directly.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image to load</p> required <code>return_format</code> <p>The format to return the image in</p> <code>'cv2'</code> <p>Returns:</p> Type Description <code>Any</code> <p>The image in the specified format</p> Source code in <code>autodistill/helpers.py</code> <pre><code>def load_image(\n    image: Any,\n    return_format=\"cv2\",\n) -&gt; Any:\n\"\"\"\n    Load an image from a file path, URI, PIL image, or numpy array.\n\n    This function is for use by Autodistill modules. You don't need to use it directly.\n\n    Args:\n        image: The image to load\n        return_format: The format to return the image in\n\n    Returns:\n        The image in the specified format\n    \"\"\"\n    if return_format not in ACCEPTED_RETURN_FORMATS:\n        raise ValueError(f\"return_format must be one of {ACCEPTED_RETURN_FORMATS}\")\n\n    if isinstance(image, Image.Image) and return_format == \"PIL\":\n        return image\n    elif isinstance(image, Image.Image) and return_format == \"cv2\":\n        # channels need to be reversed for cv2\n        return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n    elif isinstance(image, Image.Image) and return_format == \"numpy\":\n        return np.array(image)\n\n    if isinstance(image, np.ndarray) and return_format == \"PIL\":\n        return Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    elif isinstance(image, np.ndarray) and return_format == \"cv2\":\n        return image\n    elif isinstance(image, np.ndarray) and return_format == \"numpy\":\n        return image\n\n    if isinstance(image, str) and image.startswith(\"http\"):\n        if return_format == \"PIL\":\n            response = requests.get(image)\n            return Image.open(BytesIO(response.content))\n        elif return_format == \"cv2\" or return_format == \"numpy\":\n            response = requests.get(image)\n            pil_image = Image.open(BytesIO(response.content))\n            return np.array(pil_image)\n    elif os.path.isfile(image):\n        if return_format == \"PIL\":\n            return Image.open(image)\n        elif return_format == \"cv2\":\n            # channels need to be reversed for cv2\n            return cv2.cvtColor(np.array(Image.open(image)), cv2.COLOR_RGB2BGR)\n        elif return_format == \"numpy\":\n            pil_image = Image.open(image)\n            return np.array(pil_image)\n    else:\n        raise ValueError(f\"{image} is not a valid file path or URI\")\n</code></pre>"},{"location":"reference/utilities/#split-video-frames","title":"Split Video Frames","text":"<p>Split a video into frames and save them to a directory.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>The path to the video</p> required <code>output_dir</code> <code>str</code> <p>The directory to save the frames to</p> required <code>stride</code> <code>int</code> <p>The stride to use when splitting the video into frames</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>autodistill/helpers.py</code> <pre><code>def split_video_frames(video_path: str, output_dir: str, stride: int) -&gt; None:\n\"\"\"\n    Split a video into frames and save them to a directory.\n\n    Args:\n        video_path: The path to the video\n        output_dir: The directory to save the frames to\n        stride: The stride to use when splitting the video into frames\n\n    Returns:\n        None\n    \"\"\"\n    video_paths = sv.list_files_with_extensions(\n        directory=video_path, extensions=[\"mov\", \"mp4\", \"MOV\", \"MP4\"]\n    )\n\n    for name in tqdm(video_paths):\n        image_name_pattern = name + \"-{:05d}.jpg\"\n        with sv.ImageSink(\n            target_dir_path=output_dir, image_name_pattern=image_name_pattern\n        ) as sink:\n            for image in sv.get_video_frames_generator(\n                source_path=str(video_path), stride=stride\n            ):\n                sink.save_image(image=image)\n</code></pre>"},{"location":"reference/base-models/classification/","title":"Classification","text":"<p>             Bases: <code>BaseModel</code></p> <p>Use a foundation classification model to auto-label data.</p> Source code in <code>autodistill/classification/classification_base_model.py</code> <pre><code>@dataclass\nclass ClassificationBaseModel(BaseModel):\n\"\"\"\n    Use a foundation classification model to auto-label data.\n    \"\"\"\n\n    ontology: CaptionOntology\n\n    @abstractmethod\n    def predict(self, input: str) -&gt; sv.Classifications:\n\"\"\"\n        Run inference on the model.\n        \"\"\"\n        pass\n\n    def label(\n        self, input_folder: str, extension: str = \".jpg\", output_folder: str = None\n    ) -&gt; sv.ClassificationDataset:\n\"\"\"\n        Label a dataset and save it in a classification folder structure.\n        \"\"\"\n        if output_folder is None:\n            output_folder = input_folder + \"_labeled\"\n\n        os.makedirs(output_folder, exist_ok=True)\n\n        images_map = {}\n        detections_map = {}\n\n        files = glob.glob(input_folder + \"/*\" + extension)\n        progress_bar = tqdm(files, desc=\"Labeling images\")\n        # iterate through images in input_folder\n        for f_path in progress_bar:\n            progress_bar.set_description(desc=f\"Labeling {f_path}\", refresh=True)\n            image = cv2.imread(f_path)\n\n            f_path_short = os.path.basename(f_path)\n            images_map[f_path_short] = image.copy()\n            detections = self.predict(f_path)\n            detections_map[f_path_short] = detections\n\n        dataset = sv.ClassificationDataset(\n            self.ontology.classes(), images_map, detections_map\n        )\n\n        train_cs, test_cs = split_data(dataset, split_ratio=0.7)\n        test_cs, valid_cs = split_data(test_cs, split_ratio=0.5)\n\n        train_cs.as_folder_structure(root_directory_path=output_folder + \"/train\")\n\n        test_cs.as_folder_structure(root_directory_path=output_folder + \"/test\")\n\n        valid_cs.as_folder_structure(root_directory_path=output_folder + \"/valid\")\n\n        print(\"Labeled dataset created - ready for distillation.\")\n        return dataset\n</code></pre>"},{"location":"reference/base-models/classification/#autodistill.classification.classification_base_model.ClassificationBaseModel.label","title":"<code>label(input_folder, extension='.jpg', output_folder=None)</code>","text":"<p>Label a dataset and save it in a classification folder structure.</p> Source code in <code>autodistill/classification/classification_base_model.py</code> <pre><code>def label(\n    self, input_folder: str, extension: str = \".jpg\", output_folder: str = None\n) -&gt; sv.ClassificationDataset:\n\"\"\"\n    Label a dataset and save it in a classification folder structure.\n    \"\"\"\n    if output_folder is None:\n        output_folder = input_folder + \"_labeled\"\n\n    os.makedirs(output_folder, exist_ok=True)\n\n    images_map = {}\n    detections_map = {}\n\n    files = glob.glob(input_folder + \"/*\" + extension)\n    progress_bar = tqdm(files, desc=\"Labeling images\")\n    # iterate through images in input_folder\n    for f_path in progress_bar:\n        progress_bar.set_description(desc=f\"Labeling {f_path}\", refresh=True)\n        image = cv2.imread(f_path)\n\n        f_path_short = os.path.basename(f_path)\n        images_map[f_path_short] = image.copy()\n        detections = self.predict(f_path)\n        detections_map[f_path_short] = detections\n\n    dataset = sv.ClassificationDataset(\n        self.ontology.classes(), images_map, detections_map\n    )\n\n    train_cs, test_cs = split_data(dataset, split_ratio=0.7)\n    test_cs, valid_cs = split_data(test_cs, split_ratio=0.5)\n\n    train_cs.as_folder_structure(root_directory_path=output_folder + \"/train\")\n\n    test_cs.as_folder_structure(root_directory_path=output_folder + \"/test\")\n\n    valid_cs.as_folder_structure(root_directory_path=output_folder + \"/valid\")\n\n    print(\"Labeled dataset created - ready for distillation.\")\n    return dataset\n</code></pre>"},{"location":"reference/base-models/classification/#autodistill.classification.classification_base_model.ClassificationBaseModel.predict","title":"<code>predict(input)</code>  <code>abstractmethod</code>","text":"<p>Run inference on the model.</p> Source code in <code>autodistill/classification/classification_base_model.py</code> <pre><code>@abstractmethod\ndef predict(self, input: str) -&gt; sv.Classifications:\n\"\"\"\n    Run inference on the model.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/base-models/composed/","title":"Composed Model","text":"<p>             Bases: <code>DetectionBaseModel</code></p> <p>Run inference with a detection model then run inference with a classification model on the detected regions.</p> Source code in <code>autodistill/core/composed_detection_model.py</code> <pre><code>class ComposedDetectionModel(DetectionBaseModel):\n\"\"\"\n    Run inference with a detection model then run inference with a classification model on the detected regions.\n    \"\"\"\n\n    def __init__(\n        self,\n        detection_model,\n        classification_model,\n        set_of_marks=None,\n        set_of_marks_annotator=DEFAULT_LABEL_ANNOTATOR,\n    ):\n        self.detection_model = detection_model\n        self.classification_model = classification_model\n        self.set_of_marks = set_of_marks\n        self.set_of_marks_annotator = set_of_marks_annotator\n        self.ontology = self.classification_model.ontology\n\n    def predict(self, image: str) -&gt; sv.Detections:\n\"\"\"\n        Run inference with a detection model then run inference with a classification model on the detected regions.\n\n        Args:\n            image: The image to run inference on\n            annotator: The annotator to use to annotate the image\n\n        Returns:\n            detections (sv.Detections)\n        \"\"\"\n        detections = []\n        opened_image = Image.open(image)\n\n        detections = self.detection_model.predict(image)\n\n        if self.set_of_marks is not None:\n            labels = [f\"{num}\" for num in range(len(detections.xyxy))]\n\n            opened_image = np.array(opened_image)\n\n            annotated_frame = self.set_of_marks_annotator.annotate(\n                scene=opened_image, labels=labels, detections=detections\n            )\n\n            opened_image = Image.fromarray(annotated_frame)\n\n            opened_image.save(\"temp.jpeg\")\n\n            if not hasattr(self.classification_model, \"set_of_marks\"):\n                raise Exception(\n                    f\"The set classification model does not have a set_of_marks method. Supported models: {SET_OF_MARKS_SUPPORTED_MODELS}\"\n                )\n\n            result = self.classification_model.set_of_marks(\n                input=image, masked_input=\"temp.jpeg\", classes=labels, masks=detections\n            )\n\n            return detections\n\n        for pred_idx, bbox in enumerate(detections.xyxy):\n            # extract region from image\n            region = opened_image.crop((bbox[0], bbox[1], bbox[2], bbox[3]))\n\n            # save as tempfile\n            region.save(\"temp.jpeg\")\n\n            result = self.classification_model.predict(\"temp.jpeg\")\n\n            if len(result.class_id) == 0:\n                continue\n\n            result = result.get_top_k(1)[0][0]\n\n            detections.class_id[pred_idx] = result\n\n        return detections\n</code></pre>"},{"location":"reference/base-models/composed/#autodistill.core.composed_detection_model.ComposedDetectionModel.predict","title":"<code>predict(image)</code>","text":"<p>Run inference with a detection model then run inference with a classification model on the detected regions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The image to run inference on</p> required <code>annotator</code> <p>The annotator to use to annotate the image</p> required <p>Returns:</p> Type Description <code>sv.Detections</code> <p>detections (sv.Detections)</p> Source code in <code>autodistill/core/composed_detection_model.py</code> <pre><code>def predict(self, image: str) -&gt; sv.Detections:\n\"\"\"\n    Run inference with a detection model then run inference with a classification model on the detected regions.\n\n    Args:\n        image: The image to run inference on\n        annotator: The annotator to use to annotate the image\n\n    Returns:\n        detections (sv.Detections)\n    \"\"\"\n    detections = []\n    opened_image = Image.open(image)\n\n    detections = self.detection_model.predict(image)\n\n    if self.set_of_marks is not None:\n        labels = [f\"{num}\" for num in range(len(detections.xyxy))]\n\n        opened_image = np.array(opened_image)\n\n        annotated_frame = self.set_of_marks_annotator.annotate(\n            scene=opened_image, labels=labels, detections=detections\n        )\n\n        opened_image = Image.fromarray(annotated_frame)\n\n        opened_image.save(\"temp.jpeg\")\n\n        if not hasattr(self.classification_model, \"set_of_marks\"):\n            raise Exception(\n                f\"The set classification model does not have a set_of_marks method. Supported models: {SET_OF_MARKS_SUPPORTED_MODELS}\"\n            )\n\n        result = self.classification_model.set_of_marks(\n            input=image, masked_input=\"temp.jpeg\", classes=labels, masks=detections\n        )\n\n        return detections\n\n    for pred_idx, bbox in enumerate(detections.xyxy):\n        # extract region from image\n        region = opened_image.crop((bbox[0], bbox[1], bbox[2], bbox[3]))\n\n        # save as tempfile\n        region.save(\"temp.jpeg\")\n\n        result = self.classification_model.predict(\"temp.jpeg\")\n\n        if len(result.class_id) == 0:\n            continue\n\n        result = result.get_top_k(1)[0][0]\n\n        detections.class_id[pred_idx] = result\n\n    return detections\n</code></pre>"},{"location":"reference/base-models/detection/","title":"Detection","text":"<p>             Bases: <code>BaseModel</code></p> Source code in <code>autodistill/detection/detection_base_model.py</code> <pre><code>@dataclass\nclass DetectionBaseModel(BaseModel):\n    ontology: DetectionOntology\n\n    @abstractmethod\n    def predict(self, input: str) -&gt; sv.Detections:\n        pass\n\n    def sahi_predict(self, input: str) -&gt; sv.Detections:\n        slicer = sv.InferenceSlicer(callback=self.predict)\n\n        return slicer(load_image(input, return_format=\"cv2\"))\n\n    def _record_confidence_in_files(\n        self,\n        annotations_directory_path: str,\n        images: Dict[str, np.ndarray],\n        annotations: Dict[str, sv.Detections],\n    ) -&gt; None:\n        Path(annotations_directory_path).mkdir(parents=True, exist_ok=True)\n        for image_name, _ in images.items():\n            detections = annotations[image_name]\n            yolo_annotations_name, _ = os.path.splitext(image_name)\n            confidence_path = os.path.join(\n                annotations_directory_path,\n                \"confidence-\" + yolo_annotations_name + \".txt\",\n            )\n            confidence_list = [str(x) for x in detections.confidence.tolist()]\n            save_text_file(lines=confidence_list, file_path=confidence_path)\n            print(\"Saved confidence file: \" + confidence_path)\n\n    def label(\n        self,\n        input_folder: str,\n        extension: str = \".jpg\",\n        output_folder: str = None,\n        human_in_the_loop: bool = False,\n        roboflow_project: str = None,\n        roboflow_tags: str = [\"autodistill\"],\n        sahi: bool = False,\n        record_confidence: bool = False,\n        with_nms: bool = False,\n    ) -&gt; sv.DetectionDataset:\n\"\"\"\n        Label a dataset with the model.\n        \"\"\"\n        if output_folder is None:\n            output_folder = input_folder + \"_labeled\"\n\n        os.makedirs(output_folder, exist_ok=True)\n\n        images_map = {}\n        detections_map = {}\n\n        if sahi:\n            slicer = sv.InferenceSlicer(callback=self.predict)\n\n        files = glob.glob(input_folder + \"/*\" + extension)\n        progress_bar = tqdm(files, desc=\"Labeling images\")\n        # iterate through images in input_folder\n        for f_path in progress_bar:\n            progress_bar.set_description(desc=f\"Labeling {f_path}\", refresh=True)\n            image = cv2.imread(f_path)\n\n            f_path_short = os.path.basename(f_path)\n            images_map[f_path_short] = image.copy()\n\n            if sahi:\n                detections = slicer(f_path)\n            else:\n                detections = self.predict(f_path)\n\n            if with_nms:\n                detections = detections.with_nms()\n\n            detections_map[f_path_short] = detections\n\n        dataset = sv.DetectionDataset(\n            self.ontology.classes(), images_map, detections_map\n        )\n\n        dataset.as_yolo(\n            output_folder + \"/images\",\n            output_folder + \"/annotations\",\n            min_image_area_percentage=0.01,\n            data_yaml_path=output_folder + \"/data.yaml\",\n        )\n\n        if record_confidence is True:\n            self._record_confidence_in_files(\n                output_folder + \"/annotations\", images_map, detections_map\n            )\n        split_data(output_folder, record_confidence=record_confidence)\n\n        if human_in_the_loop:\n            roboflow.login()\n\n            rf = roboflow.Roboflow()\n\n            workspace = rf.workspace()\n\n            workspace.upload_dataset(output_folder, project_name=roboflow_project)\n\n        print(\"Labeled dataset created - ready for distillation.\")\n        return dataset\n</code></pre>"},{"location":"reference/base-models/detection/#autodistill.detection.detection_base_model.DetectionBaseModel.label","title":"<code>label(input_folder, extension='.jpg', output_folder=None, human_in_the_loop=False, roboflow_project=None, roboflow_tags=['autodistill'], sahi=False, record_confidence=False, with_nms=False)</code>","text":"<p>Label a dataset with the model.</p> Source code in <code>autodistill/detection/detection_base_model.py</code> <pre><code>def label(\n    self,\n    input_folder: str,\n    extension: str = \".jpg\",\n    output_folder: str = None,\n    human_in_the_loop: bool = False,\n    roboflow_project: str = None,\n    roboflow_tags: str = [\"autodistill\"],\n    sahi: bool = False,\n    record_confidence: bool = False,\n    with_nms: bool = False,\n) -&gt; sv.DetectionDataset:\n\"\"\"\n    Label a dataset with the model.\n    \"\"\"\n    if output_folder is None:\n        output_folder = input_folder + \"_labeled\"\n\n    os.makedirs(output_folder, exist_ok=True)\n\n    images_map = {}\n    detections_map = {}\n\n    if sahi:\n        slicer = sv.InferenceSlicer(callback=self.predict)\n\n    files = glob.glob(input_folder + \"/*\" + extension)\n    progress_bar = tqdm(files, desc=\"Labeling images\")\n    # iterate through images in input_folder\n    for f_path in progress_bar:\n        progress_bar.set_description(desc=f\"Labeling {f_path}\", refresh=True)\n        image = cv2.imread(f_path)\n\n        f_path_short = os.path.basename(f_path)\n        images_map[f_path_short] = image.copy()\n\n        if sahi:\n            detections = slicer(f_path)\n        else:\n            detections = self.predict(f_path)\n\n        if with_nms:\n            detections = detections.with_nms()\n\n        detections_map[f_path_short] = detections\n\n    dataset = sv.DetectionDataset(\n        self.ontology.classes(), images_map, detections_map\n    )\n\n    dataset.as_yolo(\n        output_folder + \"/images\",\n        output_folder + \"/annotations\",\n        min_image_area_percentage=0.01,\n        data_yaml_path=output_folder + \"/data.yaml\",\n    )\n\n    if record_confidence is True:\n        self._record_confidence_in_files(\n            output_folder + \"/annotations\", images_map, detections_map\n        )\n    split_data(output_folder, record_confidence=record_confidence)\n\n    if human_in_the_loop:\n        roboflow.login()\n\n        rf = roboflow.Roboflow()\n\n        workspace = rf.workspace()\n\n        workspace.upload_dataset(output_folder, project_name=roboflow_project)\n\n    print(\"Labeled dataset created - ready for distillation.\")\n    return dataset\n</code></pre>"},{"location":"reference/base-models/embedding/","title":"Embedding Model","text":"<p>             Bases: <code>ABC</code></p> <p>Use an embedding model to calculate embeddings for use in classification.</p> Source code in <code>autodistill/core/embedding_model.py</code> <pre><code>@dataclass\nclass EmbeddingModel(ABC):\n\"\"\"\n    Use an embedding model to calculate embeddings for use in classification.\n    \"\"\"\n\n    ontology: Ontology\n\n    def set_ontology(self, ontology: Ontology):\n\"\"\"\n        Set the ontology for the model.\n        \"\"\"\n        self.ontology = ontology\n\n    @abstractmethod\n    def embed_image(self, input: Any) -&gt; np.array:\n\"\"\"\n        Calculate an image embedding for an image.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def embed_text(self, input: Any) -&gt; np.array:\n\"\"\"\n        Calculate a text embedding for an image.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/base-models/embedding/#autodistill.core.embedding_model.EmbeddingModel.embed_image","title":"<code>embed_image(input)</code>  <code>abstractmethod</code>","text":"<p>Calculate an image embedding for an image.</p> Source code in <code>autodistill/core/embedding_model.py</code> <pre><code>@abstractmethod\ndef embed_image(self, input: Any) -&gt; np.array:\n\"\"\"\n    Calculate an image embedding for an image.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/base-models/embedding/#autodistill.core.embedding_model.EmbeddingModel.embed_text","title":"<code>embed_text(input)</code>  <code>abstractmethod</code>","text":"<p>Calculate a text embedding for an image.</p> Source code in <code>autodistill/core/embedding_model.py</code> <pre><code>@abstractmethod\ndef embed_text(self, input: Any) -&gt; np.array:\n\"\"\"\n    Calculate a text embedding for an image.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/base-models/embedding/#autodistill.core.embedding_model.EmbeddingModel.set_ontology","title":"<code>set_ontology(ontology)</code>","text":"<p>Set the ontology for the model.</p> Source code in <code>autodistill/core/embedding_model.py</code> <pre><code>def set_ontology(self, ontology: Ontology):\n\"\"\"\n    Set the ontology for the model.\n    \"\"\"\n    self.ontology = ontology\n</code></pre>"},{"location":"reference/ontologies/caption-ontology/","title":"Caption Ontology","text":"<p>             Bases: <code>DetectionOntology</code></p> Source code in <code>autodistill/detection/caption_ontology.py</code> <pre><code>@dataclass\nclass CaptionOntology(DetectionOntology):\n    promptMap: List[Tuple[str, str]]\n\n    def __init__(self, ontology: Dict[str, str]):\n        self.promptMap = [(k, v) for k, v in ontology.items()]\n\n        if len(self.promptMap) == 0:\n            raise ValueError(\"Ontology is empty\")\n\n    def prompts(self) -&gt; List[str]:\n        return super().prompts()\n\n    def classToPrompt(self, cls: str) -&gt; str:\n        return super().classToPrompt(cls)\n</code></pre>"},{"location":"reference/ontologies/embedding-ontology/","title":"Embedding Ontology","text":"<p>             Bases: <code>Ontology</code></p> Source code in <code>autodistill/core/embedding_ontology.py</code> <pre><code>@dataclass\nclass EmbeddingOntology(Ontology):\n    embeddingMap: Dict[str, np.ndarray]\n\n    def __init__(self, embeddingMap, cluster=1):\n        self.embeddingMap = embeddingMap\n\n    @classmethod\n    def process(self, model: EmbeddingModel):\n        pass\n\n    def prompts(self) -&gt; List[np.ndarray]:\n        return [prompt for prompt, _ in self.embeddingMap]\n\n    def classes(self) -&gt; List[str]:\n        return [cls for _, cls in self.embeddingMap]\n</code></pre>"},{"location":"reference/target-models/classification/","title":"Classification","text":"<p>             Bases: <code>TargetModel</code></p> Source code in <code>autodistill/classification/classification_target_model.py</code> <pre><code>class ClassificationTargetModel(TargetModel):\n    @abstractmethod\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def predict(self, input: str, confidence: float = 0.5) -&gt; sv.Classifications:\n        pass\n\n    @abstractmethod\n    def train(self):\n        pass\n</code></pre>"},{"location":"reference/target-models/detection/","title":"Detection","text":"<p>             Bases: <code>TargetModel</code></p> Source code in <code>autodistill/detection/detection_target_model.py</code> <pre><code>class DetectionTargetModel(TargetModel):\n    @abstractmethod\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def predict(self, input: str, confidence: float = 0.5) -&gt; sv.Detections:\n        pass\n\n    @abstractmethod\n    def train(self):\n        pass\n</code></pre>"},{"location":"utilities/combine-models/","title":"Combine Models","text":"<p>You can combine detection, segmentation, and classification models to leverage the strengths of each model.</p> <p>For example, consider a scenario where you want to build a logo detection model that identifies popular logos. You could use a detection model to identify logos (i.e. Grounding DINO), then a classification model to classify between the logos (i.e. Microsoft, Apple, etc.).</p> <p>To combine models, you need to choose:</p> <ol> <li>Either a detection or a segmentation model, and;</li> <li>A classification model.</li> </ol> <p>Let's walk through an example of using a combination of Grounding DINO and SAM (GroundedSAM), and CLIP for logo classification.</p> <pre><code>from autodistill_clip import CLIP\nfrom autodistill.detection import CaptionOntology\nfrom autodistill_grounded_sam import GroundedSAM\nimport supervision as sv\n\nfrom autodistill.core.custom_detection_model import CustomDetectionModel\nimport cv2\n\nclasses = [\"McDonalds\", \"Burger King\"]\n\n\nSAMCLIP = CustomDetectionModel(\n    detection_model=GroundedSAM(\n        CaptionOntology({\"logo\": \"logo\"})\n    ),\n    classification_model=CLIP(\n        CaptionOntology({k: k for k in classes})\n    )\n)\n\nIMAGE = \"logo.jpg\"\n\nresults = SAMCLIP.predict(IMAGE)\n\nimage = cv2.imread(IMAGE)\n\nannotator = sv.MaskAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [\n    f\"{classes[class_id]} {confidence:0.2f}\"\n    for _, _, confidence, class_id, _ in results\n]\n\nannotated_frame = annotator.annotate(\n    scene=image.copy(), detections=results\n)\nannotated_frame = label_annotator.annotate(\n    scene=annotated_frame, labels=labels, detections=results\n)\n\nsv.plot_image(annotated_frame, size=(8, 8))\n</code></pre> <p>Here are the results:</p> <p></p>"},{"location":"utilities/combine-models/#see-also","title":"See Also","text":"<ul> <li>Automatically Label Product SKUs with Autodistill : Uses a combination of Grounding DINO and CLIP to label product SKUs.</li> </ul>"},{"location":"utilities/combine-models/#code-reference","title":"Code Reference","text":"<p>             Bases: <code>DetectionBaseModel</code></p> <p>Run inference with a detection model then run inference with a classification model on the detected regions.</p> Source code in <code>autodistill/core/composed_detection_model.py</code> <pre><code>class ComposedDetectionModel(DetectionBaseModel):\n\"\"\"\n    Run inference with a detection model then run inference with a classification model on the detected regions.\n    \"\"\"\n\n    def __init__(\n        self,\n        detection_model,\n        classification_model,\n        set_of_marks=None,\n        set_of_marks_annotator=DEFAULT_LABEL_ANNOTATOR,\n    ):\n        self.detection_model = detection_model\n        self.classification_model = classification_model\n        self.set_of_marks = set_of_marks\n        self.set_of_marks_annotator = set_of_marks_annotator\n        self.ontology = self.classification_model.ontology\n\n    def predict(self, image: str) -&gt; sv.Detections:\n\"\"\"\n        Run inference with a detection model then run inference with a classification model on the detected regions.\n\n        Args:\n            image: The image to run inference on\n            annotator: The annotator to use to annotate the image\n\n        Returns:\n            detections (sv.Detections)\n        \"\"\"\n        detections = []\n        opened_image = Image.open(image)\n\n        detections = self.detection_model.predict(image)\n\n        if self.set_of_marks is not None:\n            labels = [f\"{num}\" for num in range(len(detections.xyxy))]\n\n            opened_image = np.array(opened_image)\n\n            annotated_frame = self.set_of_marks_annotator.annotate(\n                scene=opened_image, labels=labels, detections=detections\n            )\n\n            opened_image = Image.fromarray(annotated_frame)\n\n            opened_image.save(\"temp.jpeg\")\n\n            if not hasattr(self.classification_model, \"set_of_marks\"):\n                raise Exception(\n                    f\"The set classification model does not have a set_of_marks method. Supported models: {SET_OF_MARKS_SUPPORTED_MODELS}\"\n                )\n\n            result = self.classification_model.set_of_marks(\n                input=image, masked_input=\"temp.jpeg\", classes=labels, masks=detections\n            )\n\n            return detections\n\n        for pred_idx, bbox in enumerate(detections.xyxy):\n            # extract region from image\n            region = opened_image.crop((bbox[0], bbox[1], bbox[2], bbox[3]))\n\n            # save as tempfile\n            region.save(\"temp.jpeg\")\n\n            result = self.classification_model.predict(\"temp.jpeg\")\n\n            if len(result.class_id) == 0:\n                continue\n\n            result = result.get_top_k(1)[0][0]\n\n            detections.class_id[pred_idx] = result\n\n        return detections\n</code></pre>"},{"location":"utilities/combine-models/#autodistill.core.composed_detection_model.ComposedDetectionModel.predict","title":"<code>predict(image)</code>","text":"<p>Run inference with a detection model then run inference with a classification model on the detected regions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The image to run inference on</p> required <code>annotator</code> <p>The annotator to use to annotate the image</p> required <p>Returns:</p> Type Description <code>sv.Detections</code> <p>detections (sv.Detections)</p> Source code in <code>autodistill/core/composed_detection_model.py</code> <pre><code>def predict(self, image: str) -&gt; sv.Detections:\n\"\"\"\n    Run inference with a detection model then run inference with a classification model on the detected regions.\n\n    Args:\n        image: The image to run inference on\n        annotator: The annotator to use to annotate the image\n\n    Returns:\n        detections (sv.Detections)\n    \"\"\"\n    detections = []\n    opened_image = Image.open(image)\n\n    detections = self.detection_model.predict(image)\n\n    if self.set_of_marks is not None:\n        labels = [f\"{num}\" for num in range(len(detections.xyxy))]\n\n        opened_image = np.array(opened_image)\n\n        annotated_frame = self.set_of_marks_annotator.annotate(\n            scene=opened_image, labels=labels, detections=detections\n        )\n\n        opened_image = Image.fromarray(annotated_frame)\n\n        opened_image.save(\"temp.jpeg\")\n\n        if not hasattr(self.classification_model, \"set_of_marks\"):\n            raise Exception(\n                f\"The set classification model does not have a set_of_marks method. Supported models: {SET_OF_MARKS_SUPPORTED_MODELS}\"\n            )\n\n        result = self.classification_model.set_of_marks(\n            input=image, masked_input=\"temp.jpeg\", classes=labels, masks=detections\n        )\n\n        return detections\n\n    for pred_idx, bbox in enumerate(detections.xyxy):\n        # extract region from image\n        region = opened_image.crop((bbox[0], bbox[1], bbox[2], bbox[3]))\n\n        # save as tempfile\n        region.save(\"temp.jpeg\")\n\n        result = self.classification_model.predict(\"temp.jpeg\")\n\n        if len(result.class_id) == 0:\n            continue\n\n        result = result.get_top_k(1)[0][0]\n\n        detections.class_id[pred_idx] = result\n\n    return detections\n</code></pre>"},{"location":"utilities/compare-models/","title":"Compare Models","text":"<p>You can compare two or more models on multiple images using the <code>compare</code> function.</p> <p>This function is ideal if you want to evaluate how different models perform on a single image or multiple images.</p> <p>The following example shows how to compare OWLv2 and Grounding DINO on a single image:</p> <pre><code>from autodistill_grounding_dino import GroundingDINO\nfrom autodistill_owlv2 import OWLv2\n\nfrom autodistill.detection import CaptionOntology\nfrom autodistill.utils import compare\n\nontology = CaptionOntology(\n    {\n        \"solar panel\": \"solar panel\",\n    }\n)\n\nmodels = [\n    GroundingDINO(ontology=ontology),\n    OWLv2(ontology=ontology),\n]\n\nimages = [\n    \"./solar.jpg\"\n]\n\ncompare(\n    models=models,\n    images=images\n)\n</code></pre> <p>Here are the results:</p> <p></p> <p>Above, we can see predictions from Grounding DINO and OWLv2.</p>"},{"location":"utilities/compare-models/#code-reference","title":"Code Reference","text":"<p>Compare the predictions of multiple models on multiple images.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>list</code> <p>The models to compare</p> required <code>images</code> <code>List[str]</code> <p>The images to compare</p> required <p>Returns:</p> Type Description <p>A grid of images with the predictions of each model on each image.</p> Source code in <code>autodistill/utils.py</code> <pre><code>def compare(models: list, images: List[str]):\n\"\"\"\n    Compare the predictions of multiple models on multiple images.\n\n    Args:\n        models: The models to compare\n        images: The images to compare\n\n    Returns:\n        A grid of images with the predictions of each model on each image.\n    \"\"\"\n    image_results = []\n    model_results = []\n\n    for model in models:\n        # get model class name\n        model_name = model.__class__.__name__\n\n        for image in images:\n            results = model.predict(image)\n\n            image_data = cv2.imread(image)\n\n            image_result = plot(\n                image_data, results, classes=model.ontology.prompts(), raw=True\n            )\n\n            image_results.append(image_result)\n\n            model_results.append(model_name)\n\n    sv.plot_images_grid(\n        image_results,\n        grid_size=(len(models), len(images)),\n        titles=model_results,\n        size=(16, 16),\n    )\n</code></pre>"},{"location":"utilities/nms/","title":"Apply Non-Maximum Suppression (NMS)","text":"<p>You can apply Non-Maximum Suppression (NMS) to predictions from a detection model to remove overlapping bounding boxes.</p> <p>To do so, add <code>.with_nms()</code> to the result of any <code>predict()</code> or <code>predict_sahi()</code> method from an object detection model.</p> <p>Here is an example of running NMS on predictions from a Grounding DINO model:</p> Without NMSWith NMS <pre><code>from autodistill_owlv2 import OWLv2\nfrom autodistill.detection import CaptionOntology\nfrom autodistill.utils import plot\n\nimport cv2\n\nontology = CaptionOntology({\"person\": \"person\"})\n\nbase_model = OWLv2(ontology=ontology)\n\ndetections = base_model.predict(\"./dog.jpeg\")\n\nplot(\n    image=cv2.imread(\"./dog.jpeg\"),\n    detections=detections,\n    classes=base_model.ontology.classes(),\n)\n</code></pre> <p></p> <pre><code>from autodistill_owlv2 import OWLv2\nfrom autodistill.detection import CaptionOntology\nfrom autodistill.utils import plot\n\nimport cv2\n\nontology = CaptionOntology({\"person\": \"person\"})\n\nbase_model = OWLv2(ontology=ontology)\n\ndetections = base_model.predict(\"./dog.jpeg\")\n\nplot(\n    image=cv2.imread(\"./dog.jpeg\"),\n    detections=detections.with_nms(),\n    classes=base_model.ontology.classes(),\n)\n</code></pre> <p></p>"},{"location":"utilities/sahi/","title":"Use SAHI to Detect Objects","text":"<p>Slicing Aided Hyper Inference (SAHI) is a technique that improves the detection rate of small objects in an image. SAHI involves splitting up an image into segments, then runs inference on each segment. Then, the results from each segment are combined into a single result.</p> <p>Because SAHI runs inference on separate segments, it will take longer to run inference on an image with SAHI than without SAHI.</p> <p>You can use SAHI when running inference on a single image with Autodistill, or when using Autodistill to label a folder of images.</p>"},{"location":"utilities/sahi/#use-sahi-in-a-single-prediction","title":"Use SAHI in a Single Prediction","text":"<p>To use SAHI in a single prediction, use the <code>sahi</code> parameter in the <code>predict()</code> method:</p> <pre><code>import cv2\nimport supervision as sv\n\nfrom autodistill_grounding_dino import GroundingDINO\nfrom autodistill.detection import CaptionOntology\n\nbase_model = GroundingDINO(ontology=CaptionOntology({\"person\": \"person\"}))\n\ndetections = base_model.predict_sahi(\"./image.jpg\")\n\nclasses = [\"person\"]\n\nbox_annotator = sv.BoxAnnotator()\n\nlabels = [\n    f\"{classes[class_id]} {confidence:0.2f}\"\n    for _, _, confidence, class_id, _\n    in detections\n]\n\nimage = cv2.imread(\"./image.jpg\")\n\nannotated_frame = box_annotator.annotate(\n    scene=image.copy(),\n    detections=detections,\n    labels=labels\n)\n\nsv.plot_image(image=annotated_frame, size=(16, 16))\n</code></pre> <p>Here are the results before and after SAHI:</p> Without SAHIWith SAHI <p></p> <p></p> <p>The image processed with SAHI detected more people.</p>"},{"location":"utilities/sahi/#use-sahi-to-label-a-folder-of-images","title":"Use SAHI to Label a Folder of Images","text":"<p>To use SAHI to label a folder of images, use the <code>sahi</code> parameter in the <code>label()</code> method on any base model:</p> <pre><code>base_model.label_folder(\n    input_folder=\"./images\",\n    output_folder=\"./labeled-images\",\n    sahi=True\n)\n</code></pre>"},{"location":"utilities/sahi/#see-also","title":"See Also","text":"<ul> <li>Using SAHI with supervision</li> </ul>"},{"location":"utilities/use-embeddings-in-classification/","title":"Use Embeddings in Classification","text":"<p>You can use embeddings in an <code>EmbeddingOntology</code> to classify images and detections with Autodistill.</p> <p>This has two uses:</p> <ol> <li>Classify entire images using embeddings computed with an embedding model, and;</li> <li>Classify regions of an image using the ComposedDetectionModel API.</li> </ol> <p>This API is especially useful if a classification model with embedding support (i.e. CLIP) struggles with a text prompt you provide.</p> <p>Consider a scenario where you want to classify vinyl records. You could compute an embedding for each album cover, then use those embeddings for classification.</p> <p>There are two <code>EmbeddingOntology</code> classes:</p> <ul> <li><code>EmbeddingOntologyImage</code>: Accepts a mapping from a text prompt (the class you will use in labeling) to an embedding. The embedding model you use for labeling (i.e. CLIP) will automatically compute embeddings for each image.</li> <li><code>EmbeddingOntologyRaw</code>: Accepts a mapping from a text prompt (the class you will use in labeling) to an embedding. You must compute the embeddings yourself, then provide them to the <code>EmbeddingOntologyRaw</code> class.</li> </ul> <p>In most cases, <code>EmbeddingOntologyImage</code> is the best choice, because Autodistill handles loading the model.</p> <p>However, if you already have embeddings, <code>EmbeddingOntologyRaw</code> is a better choice.</p> <p>If you use <code>EmbeddingOntologyImage</code> with pre-computed embeddings, you must use the same embedding model as the model you use for classification in Autodistill, otherwise auto-labeling will return inaccurate results.</p>"},{"location":"utilities/use-embeddings-in-classification/#embeddingontologyimage-example","title":"EmbeddingOntologyImage Example","text":"<p>In the example below, Grounding DINO is used to detect album covers, then CLIP is used to classify the album covers.</p> <p>Six images are provided as references in the <code>EmbeddingOntologyImage</code> class. These images are embedded by CLIP, then used for classification for each vinyl record detected by Grounding DINO.</p> <p>Learn more about the ComposedDetectionModel API.</p> <pre><code>from autodistill_clip import CLIP\nfrom autodistill.detection import CaptionOntology\nfrom autodistill_grounding_dino import GroundingDINO\nfrom autodistill.core import EmbeddingOntologyImage\nfrom autodistill.core.combined_detection_model import CombinedDetectionModel\n\nimport torch\nimport clip\nfrom PIL import Image\nimport os\nimport cv2\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\nINPUT_FOLDER = \"samples\"\nDATASET_INPUT = \"./images\"\nDATASET_OUTPUT = \"./dataset\"\nPROMPT = \"album cover\"\n\nimages = os.listdir(\"samples\")\n\nimages_to_classes = {\n    \"midnights\": \"IMG_9022.jpeg\",\n    \"men amongst mountains\": \"323601467684.jpeg\",\n    \"we are\": \"IMG_9056.jpeg\",\n    \"oh wonder\": \"Images (5).jpeg\",\n    \"brightside\": \"Images (4).jpeg\",\n    \"tears for fears\": \"Images (3).jpeg\"\n}\n\nmodel = CombinedDetectionModel(\n    detection_model=GroundingDINO(\n        CaptionOntology({PROMPT: PROMPT})\n    ),\n    classification_model=CLIP(\n        EmbeddingOntologyImage(images_to_classes)\n    )\n)\n\nresult = model.predict(\"./images/example.jpeg\")\n\nplot(\n    image=cv2.imread(\"./images/example.jpeg\"),\n    detections=result\n)\n</code></pre> <p>Here is the result from inference:</p> <p></p> <p>The album cover is annotated with the label \"men amougst mountains\".</p> <p>Grounding DINO successfully identified an album cover, then our EmbeddingOntologyImage classified the album cover.</p>"},{"location":"utilities/use-embeddings-in-classification/#embeddingontologyraw-example","title":"EmbeddingOntologyRaw Example","text":"<p>In the example below, we load embeddings from a file, where embeddings are in the form <code>{prompt: embedding}</code>.</p> <pre><code>from autodistill_clip import CLIP\nfrom autodistill.detection import CaptionOntology\nfrom autodistill_grounding_dino import GroundingDINO\nfrom autodistill.core import EmbeddingOntology\nfrom autodistill.core.custom_detection_model import CustomDetectionModel\n\nimport torch\nimport clip\nfrom PIL import Image\nimport os\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\nINPUT_FOLDER = \"samples\"\nDATASET_INPUT = \"./images\"\nDATASET_OUTPUT = \"./dataset\"\nPROMPT = \"album cover\"\n\nimages = os.listdir(\"samples\")\n\nwith open(\"embeddings.json\", \"r\") as f:\n    classes_to_embeddings = json.load(f)\n\nSAMCLIP = CustomDetectionModel(\n    detection_model=GroundingDINO(\n        CaptionOntology({PROMPT: PROMPT})\n    ),\n    classification_model=CLIP(\n        EmbeddingOntology(classes_to_embeddings.items())\n    )\n)\n\nresult = model.predict(\"./images/example.jpeg\")\n\nplot(\n    image=cv2.imread(\"./images/example.jpeg\"),\n    detections=result\n)\n</code></pre>"},{"location":"utilities/use-embeddings-in-classification/#code-reference","title":"Code Reference","text":""},{"location":"utilities/use-embeddings-in-classification/#autodistill.core.embedding_ontology.compare_embeddings","title":"<code>compare_embeddings(image_embedding, comparison_embeddings, distance_metric='cosine')</code>","text":"<p>Calculate the similarity between an image embedding and all embeddings in a list.</p> <p>Parameters:</p> Name Type Description Default <code>image_embedding</code> <code>np.array</code> <p>The embedding of the image to compare.</p> required <code>comparison_embeddings</code> <code>List[np.array]</code> <p>A list of embeddings to compare against.</p> required <code>distance_metric</code> <p>The distance metric to use. Currently only supports \"cosine\".</p> <code>'cosine'</code> <p>Returns:</p> Type Description <p>A list of similarity scores.</p> Source code in <code>autodistill/core/embedding_ontology.py</code> <pre><code>def compare_embeddings(\n    image_embedding: np.array,\n    comparison_embeddings: List[np.array],\n    distance_metric=\"cosine\",\n):\n\"\"\"\n    Calculate the similarity between an image embedding and all embeddings in a list.\n\n    Args:\n        image_embedding: The embedding of the image to compare.\n        comparison_embeddings: A list of embeddings to compare against.\n        distance_metric: The distance metric to use. Currently only supports \"cosine\".\n\n    Returns:\n        A list of similarity scores.\n    \"\"\"\n    if distance_metric == \"cosine\":\n        comparisons = []\n\n        for comparison_embedding in comparison_embeddings:\n            comparisons.append(\n                cosine_similarity(\n                    image_embedding.reshape(1, -1), comparison_embedding.reshape(1, -1)\n                ).flatten()\n            )\n\n        return sv.Classifications(\n            class_id=np.array([i for i in range(len(comparisons))]),\n            confidence=np.array(comparisons).flatten(),\n        )\n    else:\n        raise NotImplementedError(\n            f\"Distance metric {distance_metric} is not supported.\"\n        )\n</code></pre>"},{"location":"utilities/visualize-predictions/","title":"Visualize Predictions","text":"<p>The <code>plot()</code> method allows you to visualize predictions from a detection or segmentation model.</p> <p>If you use a detection model to run inference (i.e. Grounding DINO), the <code>plot()</code> method will plot bounding boxes for each prediction.</p> <p>If you use a segmentation model to run inference (i.e. Grounded SAM), the <code>plot()</code> method will plot segmentation masks for each prediction. </p> <p>Here is an example of the method used to annotate predictions from a Grounding DINO model:</p> Bounding BoxSegmentation Mask <pre><code>from autodistill_grounding_dino import GroundingDINO\nfrom autodistill.detection import CaptionOntology\nfrom autodistill.utils.plot import plot\nimport cv2\n\nontology = CaptionOntology(\n    {\n        \"dog\": \"dog\",\n    }\n)\n\nmodel = GroundingDINO(ontology=ontology)\n\nresult = model.predict(\"./dog.jpeg\")\n\nplot(\n    image=cv2.imread(\"./dog.jpeg\"),\n    classes=base_model.ontology.classes(),\n    detections=result\n)\n</code></pre> <p></p> <pre><code>from autodistill_grounded_sam import GroundedSAM\nfrom autodistill.detection import CaptionOntology\nfrom autodistill.utils import plot\nimport cv2\n\nontology = CaptionOntology(\n    {\n        \"dog\": \"dog\",\n    }\n)\n\nmodel = GroundedSAM(ontology=ontology)\n\nresult = model.predict(\"./dog.jpeg\")\n\nplot(\n    image=cv2.imread(\"./dog.jpeg\"),\n    classes=model.ontology.classes(),\n    detections=result\n)\n</code></pre> <p></p>"},{"location":"utilities/visualize-predictions/#code-reference","title":"Code Reference","text":"<p>Plot bounding boxes or segmentation masks on an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p>The image to plot on</p> required <code>detections</code> <p>The detections to plot</p> required <code>classes</code> <code>List[str]</code> <p>The classes to plot</p> required <code>raw</code> <p>Whether to return the raw image or plot it interactively</p> <code>False</code> <p>Returns:</p> Type Description <p>The raw image (np.ndarray) if raw=True, otherwise None (image is plotted interactively</p> Source code in <code>autodistill/utils.py</code> <pre><code>def plot(image: np.ndarray, detections, classes: List[str], raw=False):\n\"\"\"\n    Plot bounding boxes or segmentation masks on an image.\n\n    Args:\n        image: The image to plot on\n        detections: The detections to plot\n        classes: The classes to plot\n        raw: Whether to return the raw image or plot it interactively\n\n    Returns:\n        The raw image (np.ndarray) if raw=True, otherwise None (image is plotted interactively\n    \"\"\"\n    # TODO: When we have a classification annotator\n    # in supervision, we can add it here\n    if detections.mask is not None:\n        annotator = sv.MaskAnnotator()\n    else:\n        annotator = sv.BoxAnnotator()\n\n    label_annotator = sv.LabelAnnotator()\n\n    labels = [\n        f\"{classes[class_id]} {confidence:0.2f}\"\n        for _, _, confidence, class_id, _ in detections\n    ]\n\n    annotated_frame = annotator.annotate(scene=image.copy(), detections=detections)\n    annotated_frame = label_annotator.annotate(\n        scene=annotated_frame, labels=labels, detections=detections\n    )\n\n    if raw:\n        return annotated_frame\n\n    sv.plot_image(annotated_frame, size=(8, 8))\n</code></pre>"}]}